---
weight: 1
bookFlatSection: true
title: "Syllabus"
bookHidden: false
---


<!--

take this course a little more easy

-->

# Syllabus

Activities

- Data collection pitch (group) / presentation
- Legal battle + anonymization outcome
- Prototype
- Deployment as large project

session chair, presentations, pitches, discussions

- Need something like a watch list before, and then practice inbetween
- WHY is it important - how to use these concepts inbetween?


## Preparation before the course starts

- Software environment setup
- Introduction to R and RStudio
- Overview about this course

## During the course

- Module 1: Data engineering for marketing research
  - Kick-off
    - Overview about data types (Wedel & Kannan)
    - Data providers/sources
    - Conceptual framework of course: pipelines / modules + workflow design;  input/transformation/output; management of code + designing a workflow from scratch
  - Self-study
    - Mela: pitching for data from a business


<!--Prepare data sets for empirical analysis and manage research projects efficiently-->

- Module 2: Data formats and data principles
  - Unit of analysis
  - Data types and file formats: databases versus files
  - Raw data versus derived data sets in a workflow

  - Tutorial: Reading in data & inspecting data / basic SQL
    - Data formats: file-based versus data-bases
      - Files:
        - Flat versus program + reading in
      - Databases [focus, similar to SQL]
        - Structured versus unstructured + reading in
    - Unit of analysis and primary keys


<!--Use R to read in various data formats for further processing
-->

- Module 3: Common data operations and cleaning data
  - Apply common data operations in R to transform and clean your data (e.g., aggregation, merging, de-duplication, reshaping, date conversions, regular expressions)

    - Tutorial / exercises:   - Common data operations
  Common data operations

  - Use basic programming concepts to increase speed and minimize errors (e.g., looping, vectorization, writing functions, handling errors/debugging)

- Module 4: Feature engineering
  - Operationalize variables/engineer features from numerical, textual, and visual raw data

- Module 5: Store and manage data using file-based systems and databases

- Module 6: Workflow management
  - Use workflow management techniques to create and audit automated and reproducible data pipelines
  - Version code and manage and contribute to GitHub repositories

- Module 7: Document and archive final data sets, and learn how to make them available for public (re)use



----

## Meetings



## Online Tutorials (prerecorded) / "labs"

### Webscraping 101

### Webscraping Advanced

### APIs 101

### APIs Advanced

### Structured databases

### Unstructured databases

### Disclosing data / data workflow

## Web scraping

## APIs

- Software environment setup
- Introduction to Anaconda Python (Jupyter Notebook, Spyder)
- Introduction to the Glossary

## Web scraping

- Data retrieval from websites
- Application protocol interfaces
- Data/functionality/algorithm access
- API get
- API provide
- Releasing/sharing data via APIs
- Parsing

## Data management

- Database technology
- Structured: MySQL, Google BigQuery
- Unstructured: Amazon DynamoDB, MongoDB
- File-based systems
- Local: SSD, HDD
- Server/cloud: S3, FTP, Google Drive
- Per database
- Schema/design
- Extracting and writing data
- Indexing
- Maintenance
- ShinyApps / Interactive dashboards


- Scraper 1: Static scraper, single-machine, no database (two versions: either with or without browser); backward-looking versus forward-looking
- Scraper 2: Dynamic scraper, with database connection + monitoring

## Ethics

## Topics for the guide

### Web scraping

- Design principles
- Connecting to a website without browser window
- Connecting to a website with browser window
- Using headers
- Cookies and continuing existing sessions
- Identifying objects using CSS selectors
- Identifying objects using XPATH selectors
- Loop through objects
- Timing / delays
- Looping
- Planning a data collection
- Seeding
- Sampling
- Writing to CSV
- Writing to JSON

### Saving and writing locally and remotely (databases, file-based systems)
- Writing to file
- Writing to S3
- SQL - write
- SQL - read
-

Project: Data collection + auditing of data

Real-time analytics (use database (learnt here), in combination with research method (e.g., regression), to create insights in realtime

Project: collect data OR make available data/algorithm from other classes / how to run supervision?
