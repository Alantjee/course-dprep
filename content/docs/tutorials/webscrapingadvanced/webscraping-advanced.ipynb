{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Advanced (oDCM)\n",
    "\n",
    "*We finally move towards some real-world examples: Twitter & Instagram! You'll quickly realize that both websites face some extra challenges in terms of scraping. We'll show you how to overcome them, and build a scraper that you can use for your own project!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to: \n",
    "* Understand the difference between headless and browser emulation and ability to apply both methods (using Selenium)\n",
    "* Emulate user interaction with a site using timers, clicks, scrolling, and filling in forms \n",
    "* Access data that is hidden behind a login-screen\n",
    "* Apply search parameters to obtain subsets of data\n",
    "* Scrape and store images from the internet locally\n",
    " \n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This course draws on a variety of online resources that can be retrieved from the [course website](https://odcm.hannesdatta.com/docs/about/).\n",
    "\n",
    "--- \n",
    "\n",
    "## Contact\n",
    "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Selenium \n",
    "\n",
    "### 1.1 Why Selenium? \n",
    "In the Web Scraping 101 tutorial, we used BeautifulSoup to turn HTML into a data structure that we could search and access using Python-like syntax. While it's easy to get started with this library, it has limitations when it comes to dynamic websites. That is, websites of which the content changes after each page refresh. Selenium can handle both static and dynamic websites and mimic user behavior (e.g., scrolling, clicking, logging in). It launches another web browser window in which all actions are visible which makes it feel more intuitive. For example, the video below launches a regular Google Chrome window and visits [`instagram.com`](https://www.instagram.com). This browser window behaves like normal, so you can click on buttons and fill out fields. Yet you can distinguish it from your normal web browser by the header that indicates that Chrome is being controlled by automated test software. Before you can try it out yourself, we need to install some additional software which we'll explain next. \n",
    "\n",
    "<img src=\"images/selenium_instagram.gif\" align=\"left\" width=70%/>\n",
    "\n",
    "\n",
    "<!-- \n",
    "\n",
    "With Selenium we use a standard called \"XPath\" to navigate through an HTML document: this is the official tutorial for working with XPath. The syntax is different, but the intuition is similar: we can find a parent node by its attribute (class, id, etc.) and then navigate down the tree to its children.\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing Selenium\n",
    "You will need to (1) install the Python package for Selenium, (2) download a web driver to interface with a web browser, and (3) configure Selenium to recognize your web driver.\n",
    "1. Open Anaconda Prompt (Windows) or the Terminal (Mac), type the command `conda install selenium`, and agree to whatever the package manager wants to install or update (usually by pressing `y` to confirm your choice). \n",
    "2. Once we run the scraper, a Chrome browser launches which requires a web driver executable file. Download this file from [here](https://sites.google.com/a/chromium.org/chromedriver/downloads) (open [this](https://www.whatismybrowser.com/detect/what-version-of-chrome-do-i-have) site in Chrome to identify your current Chrome version). \n",
    "3. Unzip the file and move it to the same directory where you're running this notebook. Make a note of the path to this directory as you'll need to reference it later. The path to my Chrome driver looks like this (Mac): `/Users/royklaassebos/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver`. On Windows, it may look like this: `E:/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver.exe`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `chrome_path` variable to the location where you've stored the driver and run the cell. It should open an empty Chrome window (don't close it until you're done with scraping!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver\n",
    "\n",
    "chrome_path = \"/Users/royklaassebos/Google Drive/2020/Data_Scientist_Tilburg/oDCM/chromedriver\"\n",
    "driver = selenium.webdriver.Chrome(executable_path = chrome_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Follow the steps and make sure it works properly on your machine. What happens once you run the cell above twice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Access Sites Programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**  \n",
    "Next, we're going to tell the browser to visit the Tilburg University Twitter account. We call the `driver` object we created above and use the `get` method, which we pass the URL of the website we'd like to extract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://twitter.com/TilburgU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/twitter_tilburgu.png\" align=\"left\" width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "As most information can only be obtained once you're signed in, manually login to your Twitter account through the driver page (create a new account if you don't have one yet - or if you don't want to run the risk of getting blocked on your personal account). \n",
    "\n",
    "From this point, we can use BeautifulSoup as we learned previously, though we create the `res` object from the `driver` object this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you inspect the HTML code of the Twitter page you'll discover that the class names are more complex than the ones we looked at earlier. Take a look at the gigantic class name of the Twitter bio, for example..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/twitter_bio.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which we can extract using the class name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow @TilburgU and we will keep you up to date on our latest news! Our webcare team is more than happy to answer your questions on work days, 9 AM - 5 PM.'"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(class_ = \"css-901oao r-jwli3a r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-qvutc0\")[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or by filtering on the `data-testid` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow @TilburgU and we will keep you up to date on our latest news! Our webcare team is more than happy to answer your questions on work days, 9 AM - 5 PM.'"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(attrs={\"data-testid\": \"UserDescription\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**  \n",
    "Using the same approach as above, extract the (i) number of followers, (ii) the location, and the (iii) join date of the [TilburgU](https://twitter.com/tilburgU) Twitter account. Tip: use Google Inspector to determine an appropriate navigation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Followers: 13K \n",
      "Location: Tilburg, The Netherlands \n",
      "Join date: Joined June 2009\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "followers = soup.find_all(class_ = \"css-901oao css-16my406 r-jwli3a r-1qd0xha r-b88u0q r-ad9z0x r-bcqeeo r-qvutc0\")[1].text \n",
    "location = soup.find(attrs={\"data-testid\": \"UserProfileHeader_Items\"}).find_all('span')[1].text\n",
    "join_date = soup.find(attrs = {\"data-testid\": \"UserProfileHeader_Items\"}).find_all('span')[3].text\n",
    "\n",
    "print(f\"Followers: {followers} \\nLocation: {location} \\nJoin date: {join_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Scroll Sites Programmatically\n",
    "\n",
    "**Importance**  \n",
    "In a similar way, we can scrape the content of the most recent tweet as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't have any plans for New Year's Eve yet? Join the Tilburg University pub quiz on December 31, 20:00 hrs. Make sure to register before December 31, 10:00 hrs. https://tilburguniversity.edu/current/events/cristmas-and-new-years-activities…\""
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[0].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for older tweets we simply increment the counter by one: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We wish you Happy Holidays and a Wonderful New Year!  Our university will be closed for a couple of days: https://tilburguniversity.edu/contact/openinghours… \\nAs our webcare team is also celebrating the holidays, response time will be longer than usual. We will be back in full capacity on Jan 4, 2021!'"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[1].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy right? Not so fast.. From the 10th tweet onwards (in your case it may be a different figure; dependent on screen size, resolution, etc.), it returns an `IndexError: list index out of range`. This is because Twitter only pulls in new tweets once you scroll down the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Congratulations to @MicheleNuijten for winning the Young eScientist Award 2020 from the @eScienceCenter in the amount of 50,000 euro! She will further develop statcheck, an open-access tool for detecting statistical reporting errors. http://tilburguniversity.edu/current/news/more-news/michele-nuijten-escientist-award-meta-research-statcheck…'"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9th tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[8].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-535-d5b04f8bca6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 10th tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data-testid\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 10th tweet\n",
    "soup.find_all(attrs={\"data-testid\": \"tweet\"})[9].find_all(attrs={\"dir\": \"auto\"})[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we need to scroll down to the bottom of the page if we like to obtain more than a few tweets. Every time you run the cell below it loads another 5-10 tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Try running the cell above a couple of times. What happens to the most recent tweets? If you run the cells again that extract the 1st, 2nd, 9th, and 10th tweet, does the output change? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we need to recreate the `res` object after each iteration because the HTML code changes once you scroll down (older tweets are added and newer ones are hidden). The number of tweets in the view deviates depending on the type of media (e.g., images take up more space than text). Therefore, we first determine the number of views in the current view to make sure we capture all tweets. After we stored the last tweet in the view, we scroll down the page and start all over again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "tweets = []\n",
    "\n",
    "for _ in range(5):\n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    \n",
    "    # total number of tweets in current view\n",
    "    num_tweets_view = len(soup.find_all(attrs={\"data-testid\": \"tweet\"}))\n",
    "        \n",
    "    # add tweets to list\n",
    "    for counter in range(num_tweets_view):\n",
    "        tweets.append(soup.find_all(attrs={\"data-testid\": \"tweet\"})[counter].find_all(attrs={\"dir\": \"auto\"})[4].text)\n",
    "    \n",
    "    # scroll down the page\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    \n",
    "    # pause for 5 seconds\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "1. What happens once you first scroll down the page and then run the cell above? Does `tweets` differ? Why? \n",
    "2. Estimate how many times you would need to scroll in order to capture all tweets (tip: you find the total number of tweets at the top). By the way, there's no need to collect all tweets!\n",
    "3. Write a function `process_tweets()` that returns a list of dictionaries in which each dictionary contains the original tweet, a list of mentions (e.g., `@gemeentetilburg`), and a list of hashtags (e.g., `#makeitintilburg`). Tip: you may first want to split each tweet into a list of words and work from there. When is a word considered a hashtag? And a mention? How about punctuation? Test your function with the list of `tweets` above. \n",
    "4. What's the most-used hashtag in the `tweets` dataset? Start with the output of `preprocess_tweets(tweets)`. Tip: use the [Counter](https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item) module to easily determine the number of occurrences of each hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "1. The scraper will start from the current view. Since more recent tweets are hidden as you scroll down, the scraper would skip the first few tweets in that case. \n",
    "2. Scrolling down five times yielded 56 tweets, so it would take about 3986/56 = 71 times on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "def process_tweets(tweets):\n",
    "    output = []\n",
    "    \n",
    "    for tweet in tweets: \n",
    "        mentions = []\n",
    "        hashtags = []\n",
    "        \n",
    "        # a more elegant solution can be achieved using regular expressions (outside the scope of this course)\n",
    "        \n",
    "        # remove punctuation (to avoid #hashtag?, #hashtag!, etc.)\n",
    "        for character in [\"?\", \".\", \",\", \"!\"]:\n",
    "            tweet_clean = tweet.replace(character, \"\")\n",
    "            tweet = tweet_clean\n",
    "            \n",
    "        # separate words chained by an enter with a space (to avoid #hashtag\\nABCDEF)\n",
    "        tweet = tweet.replace(\"\\n\", \" \")\n",
    "        \n",
    "        for word in tweet.split(\" \"): \n",
    "            try: \n",
    "                if word[0] == \"@\" and word.count(\"@\") == 1: \n",
    "                    mentions.append(word)\n",
    "                if word[0] == \"#\" and word.count(\"#\") == 1: \n",
    "                    hashtags.append(word) \n",
    "            except: \n",
    "                pass\n",
    "            \n",
    "        output.append({\n",
    "            \"tweet\": tweet,\n",
    "            \"mentions\": mentions,\n",
    "            \"hashtags\": hashtags\n",
    "        })\n",
    "        \n",
    "    return output\n",
    "\n",
    "tweets_processed = process_tweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'#klimaatneutrale': 1,\n",
       "         '#economie': 1,\n",
       "         '#economische': 1,\n",
       "         '#groei': 1,\n",
       "         '#TilburgU': 12,\n",
       "         '#IkPasBattle': 1,\n",
       "         '#battle': 1,\n",
       "         '#HealthyCampus:': 1,\n",
       "         '#2': 1,\n",
       "         '#statcheck': 1,\n",
       "         '#Covid_19': 2,\n",
       "         '#SGTilburg': 1,\n",
       "         '#sgtilburg': 1,\n",
       "         '#patiënt': 1,\n",
       "         '#zorgverlening': 1,\n",
       "         '#needles': 1,\n",
       "         '#makeitintilburg': 1,\n",
       "         '#ondernemerschap': 1,\n",
       "         '#entrepreneurship': 1,\n",
       "         '#digitization': 1,\n",
       "         '#AI': 2,\n",
       "         '#corona': 1,\n",
       "         '#cancer': 1,\n",
       "         '#WDPD2020': 1,\n",
       "         '#RecognitionandRewards': 1,\n",
       "         '#remembering': 1,\n",
       "         '#MidpointBrabant': 1,\n",
       "         '#coronavirus': 1,\n",
       "         '#Gebarentaal': 1,\n",
       "         '#TrusTee': 1,\n",
       "         '#TilburgUniversityMagazine': 1,\n",
       "         '#CoronaMelder': 1,\n",
       "         '#NobelPeacePrize': 1})"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 4\n",
    "from collections import Counter\n",
    "\n",
    "hashtags = []\n",
    "df = process_tweets(tweets)\n",
    "\n",
    "# add all hashtags of all tweets to a list \n",
    "for tweet_dict in df: \n",
    "    hashtags.extend(tweet_dict['hashtags'])\n",
    "    \n",
    "# count frequencies of hashtags\n",
    "Counter(hashtags)  # so the answer is: #TilburgU (what a surprise..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**  \n",
    "Retrieve the list of all accounts Tilburg University is following on Twitter (at current: 110). Store their full name, Twitter handle, and biography as a list of dictionaries. What percentage of those accounts use the word `Professor` or `professor` in their bio? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution (part I)\n",
    "from time import sleep\n",
    "\n",
    "def twitter_followings():\n",
    "    users = []\n",
    "\n",
    "    for _ in range(5): # we set the range to 10 because each view contains 10-15 accounts and we know that there are approximately 100 accounts in total\n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "        # if you don't specify the primary column it will also scrape the accounts below \"Who to follow\" (right sidebar)\n",
    "        data = soup.find(attrs={\"data-testid\": \"primaryColumn\"}).find_all(attrs={\"data-testid\": \"UserCell\"})\n",
    "\n",
    "        for counter in range(len(data)):\n",
    "            user = data[counter].find_all(\"span\")\n",
    "\n",
    "            full_name = user[1].text\n",
    "            handle = user[2].text\n",
    "\n",
    "            # not all users have a bio \n",
    "            try: \n",
    "                bio = user[5].text\n",
    "            except: \n",
    "                bio = None\n",
    "\n",
    "            user_data = {\"full_name\": full_name, \n",
    "                          \"handle\": handle,\n",
    "                          \"bio\": bio\n",
    "                         }\n",
    "                \n",
    "            if user_data not in users: \n",
    "                users.append(user_data)\n",
    "                \n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        sleep(10)\n",
    "        \n",
    "    return users\n",
    "              \n",
    "# don't forget to manually login to your Twitter account to access the following list before proceeding\n",
    "driver.get(\"https://twitter.com/TilburgU/following\") \n",
    "\n",
    "# add a short pause here, or Twitter recognizes that it's a scraper and will block your request!\n",
    "sleep(5)\n",
    "\n",
    "followings = twitter_followings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of professors among the list of account Tilburg University follows is 6.7%.\n"
     ]
    }
   ],
   "source": [
    "# solution (part II)\n",
    "professor_count = 0 \n",
    "\n",
    "for user in followings: \n",
    "    if user.get(\"bio\"):  # first check if the user has a bio at all\n",
    "        if \"professor\" in user[\"bio\"].lower():\n",
    "            professor_count += 1\n",
    "\n",
    "print(f\"The percentage of professors among the list of account Tilburg University follows is {round(professor_count / len(followings) * 100, 1)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have either picked an arbitrary value for the number of scrolls, or - at best - we have approximated the number of times based on the total number of records. An alternative strategy is based on the idea that the current position on the page remains the same if you're already at the bottom of the page and still try to scroll down. Simply put, if scrolling down changes the current position then we're not at the bottom of the page yet. \n",
    "\n",
    "With this idea in mind, we can implement this procedure with a `while` loop that remains true as long as we have not reached the end. Once the `current_height` equals the height before scrolling down (`last_height`), we `break` out of the loop and print the total number of scrolls (`scroll_counter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current height: 315579\n",
      "last height: 822\n",
      "current height: 822\n",
      "last height: 5070\n",
      "current height: 5070\n",
      "last height: 12447\n",
      "current height: 12447\n",
      "last height: 22586\n",
      "current height: 22586\n",
      "last height: 32132\n",
      "current height: 32132\n",
      "last height: 39559\n",
      "current height: 39559\n",
      "last height: 46180\n",
      "current height: 46180\n",
      "last height: 55009\n",
      "current height: 55009\n",
      "last height: 62721\n",
      "current height: 62721\n",
      "last height: 69939\n",
      "current height: 69939\n",
      "last height: 78717\n",
      "current height: 78717\n",
      "last height: 86337\n",
      "current height: 86337\n",
      "last height: 93333\n",
      "current height: 93333\n",
      "last height: 100395\n",
      "current height: 100395\n",
      "last height: 107452\n",
      "current height: 107452\n",
      "last height: 114082\n",
      "current height: 114082\n",
      "last height: 120665\n",
      "current height: 120665\n",
      "last height: 129891\n",
      "current height: 129891\n",
      "last height: 138685\n",
      "current height: 138685\n",
      "last height: 146016\n",
      "current height: 146016\n",
      "last height: 154576\n",
      "current height: 154576\n",
      "last height: 161727\n",
      "current height: 161727\n",
      "last height: 170538\n",
      "current height: 170538\n",
      "last height: 178213\n",
      "current height: 178213\n",
      "last height: 185189\n",
      "current height: 185189\n",
      "last height: 194915\n",
      "current height: 194915\n",
      "last height: 201731\n",
      "current height: 201731\n",
      "last height: 207931\n",
      "current height: 207931\n",
      "last height: 217468\n",
      "current height: 217468\n",
      "last height: 225244\n",
      "current height: 225244\n",
      "last height: 232166\n",
      "current height: 232166\n",
      "last height: 239662\n",
      "current height: 239662\n",
      "last height: 245850\n",
      "current height: 245850\n",
      "last height: 254612\n",
      "current height: 254612\n",
      "last height: 260384\n",
      "current height: 260384\n",
      "last height: 269070\n",
      "current height: 269070\n",
      "last height: 276981\n",
      "current height: 276981\n",
      "last height: 283875\n",
      "current height: 283875\n",
      "last height: 291754\n",
      "current height: 291754\n",
      "last height: 298742\n",
      "current height: 298742\n",
      "last height: 304301\n",
      "current height: 304301\n",
      "last height: 312896\n",
      "current height: 312896\n",
      "last height: 317976\n",
      "current height: 317976\n",
      "The number scrolls required to scrape all tweets is: 43\n"
     ]
    }
   ],
   "source": [
    "scroll_counter = 0 \n",
    "last_height = 0\n",
    "driver.get(\"https://twitter.com/TilburgU\")\n",
    "\n",
    "# running this cell may take a minute or two\n",
    "while True:    \n",
    "    print(\"current height: \" + str(current_height))\n",
    "    current_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "    if current_height == last_height:\n",
    "        break\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    scroll_counter += 1\n",
    "    last_height = current_height\n",
    "    print(\"last height: \" + str(last_height))\n",
    "    sleep(1)\n",
    "    \n",
    "print(f\"The number scrolls required to scrape all tweets is: {scroll_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**  \n",
    "Extend your answer of Exercise 3 so that the number of scroll is dynamic. That is, it amounts the minimum number of scrolls required to capture all followings and updates its value once new accounts are followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "from time import sleep\n",
    "\n",
    "def twitter_followings_while():\n",
    "    users = []\n",
    "    current_height = 0\n",
    "    last_height = 0\n",
    "    \n",
    "    while True: \n",
    "        current_height = driver.execute_script('return document.body.scrollHeight')\n",
    "              \n",
    "        if current_height == last_height: \n",
    "            break\n",
    "            \n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        data = soup.find(attrs={\"data-testid\": \"primaryColumn\"}).find_all(attrs={\"data-testid\": \"UserCell\"})\n",
    "\n",
    "        for counter in range(len(data)):\n",
    "            user = data[counter].find_all(\"span\")\n",
    "\n",
    "            full_name = user[1].text\n",
    "            handle = user[2].text\n",
    "\n",
    "            # not all users have a bio \n",
    "            try: \n",
    "                bio = user[5].text\n",
    "            except: \n",
    "                bio = None\n",
    "\n",
    "            # avoid duplicates\n",
    "            user_data = {\"full_name\": full_name, \n",
    "                          \"handle\": handle,\n",
    "                          \"bio\": bio\n",
    "                         }\n",
    "                \n",
    "            if user_data not in users: \n",
    "                users.append(user_data)\n",
    "\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')       \n",
    "        last_height = current_height\n",
    "        sleep(5)\n",
    "        \n",
    "    return users\n",
    "              \n",
    "# don't forget to manually login to your Twitter account to access the following list before proceeding\n",
    "driver.get(\"https://twitter.com/TilburgU/following\") \n",
    "\n",
    "# add a short pause here, or Twitter recognizes that it's a scraper and will block your request!\n",
    "sleep(1)\n",
    "\n",
    "followings_while = twitter_followings_while()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Search Tweets\n",
    "\n",
    "**Importance**  \n",
    "Although scraping data from a specific Twitter account is a good starting point, you may come across a scenario in which the tweets you're after come from a variety of sources. We can use Twitter's advanced search functionality to filter down on those tweets we're looking for.   \n",
    "\n",
    "On Twitter you find a search bar at the top which allows you to search for a topic. In the right sidebar a search filter panel appears that includes `Advanced search` tools. Filling out any of the search boxes will automatically update the user input in the search box above. For example, tweets that contain either `cats` or `dogs` (or both) can be obtained with the search query `(cats OR dogs)`. The specified search term may occur in the user name, handle, bio, tweet, or in any of the replies of a thread. All search queries are case insensitive, so `cats` and `Cats` is considered equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cats_dogs.gif\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice the URL was updated in accordance with our search parameters (remember from [icanhazdadjoke](https://icanhazdadjoke.com/api))? In fact, the new URL became: `https://twitter.com/search?q=(cats%20OR%20dogs)&src=typed_query`: \n",
    "* `q=` stands for search query\n",
    "* `(cats%20OR%20dogs)` corresponds with the contents of the query `(cats OR dogs)`\n",
    "* `&src=typed_query` indicates that we filled out the search query manually\n",
    "\n",
    "Note that `%20` is a space character. A full list of search commands and syntax can be found below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Command | Syntax | Interpretation | URL suffix | \n",
    "| :------- | :------- | :------ | :----- | \n",
    "| All of these words | `cats dogs` | Contains both `cats` and `dogs` | `cats%20dogs` | \n",
    "| Exact phrase | `\"cats\"` | Contains the exact phrase `cats` | `%22cats%22` | \n",
    "| Any of these words | `(cats OR dogs)` | Contains either `cats` or `dogs` (or both) | `(cats%20OR%20dogs)` | \n",
    "| None of these words | `-cats` | Does not contain `cats` | `-cats` | \n",
    "| These hashtags | `(#cats)` | Contains the hashtag `#cats` | `(%23cats)` | \n",
    "| Language | `lang:nl` | Tweets in specified language (default: all) | `lang%3Anl` | \n",
    "| From these accounts | `(from:hannesdatta)` | Tweets from Hannes Datta | `(from%3Ahannesdatta)` | \n",
    "| Replies to these accounts | `(to:hannesdatta)` | Replies to tweets from Hannes Datta | `(to%3Ahannesdatta)` | \n",
    "| Mentioning these accounts | `(@hannesdatta)` | Tweets mentioning Hannes Datta | `(%40hannesdatta)` | \n",
    "| Minimum replies | `min_replies:10` | Tweets with at least 10 replies | `min_replies%3A10` | \n",
    "| Minimum likes | `min_faves:10` | Tweets with at least 10 likes | `min_faves%3A10` | \n",
    "| Minimum retweets | `min_retweets:10` | Tweets with at least 10 retweets | `min_retweets%3A10` | \n",
    "| From (date) | `since:01-01-2020` | Tweets after the 1st of January 2020 | `since%3A01-01-2020` | \n",
    "| To (date) | `until:01-01-2020` | Tweets before the 1st of January 2020 | `until%3A01-01-2020` | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Play around with the filters until you get the hang of it. How do you chain multiple search commands? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**  \n",
    "1. Suppose that you're responsible for Tilburg University's Public Relations & Digital Communication and want to keep an eye on what others are writing about the university. Compile a search query in the Twitter web interface to collect tweets that refer to either `Tilburg University` (English) or `Tilburg Universiteit` (Dutch). Take note of the search command and keep track of the URL.\n",
    "2. Write a function `search_tweets()` that takes a Twitter search URL and returns a list of all tweets (including a link to the original tweet so that your PR colleague can respond if necessary). Test your function with the URL of the previous question. \n",
    "3. After further inspection, you come to the conclusion that your data include a dozen or so train (Dutch: \"trein\") disruption alerts related to \"Tilburg Universiteit\" (e.g., see example below). How can you easily exclude those tweets upfront? \n",
    "\n",
    "<img src=\"images/train_alerts.png\" align=\"left\" width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*English: #NS Resolved: Tilburg University-Eindhoven: broken train. Boxtel-Eindhoven C.: train traffic has resumed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 and 2\n",
    "def search_tweets(url):\n",
    "    driver.get(url) \n",
    "    tweets = []\n",
    "    last_height = 0\n",
    "    current_height = 0\n",
    "\n",
    "    while True: \n",
    "        current_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "        if current_height == last_height: \n",
    "            break\n",
    "\n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        data = soup.find(attrs={\"data-testid\": \"primaryColumn\"}).find_all(attrs={\"data-testid\":\"tweet\"})\n",
    "\n",
    "        for counter in range(len(data)):\n",
    "            tweet = data[counter].find_all(attrs={\"dir\":\"auto\"})\n",
    "\n",
    "            text = tweet[4].text\n",
    "            link = tweet[3]['href']\n",
    "\n",
    "            tweet_data = {\"tweet\": text, \n",
    "                          \"link\": \"https://twitter.com/\" + link\n",
    "                         }\n",
    "\n",
    "            if tweet_data not in tweets: \n",
    "                tweets.append(tweet_data)\n",
    "\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')       \n",
    "        last_height = current_height\n",
    "        sleep(5)\n",
    "        \n",
    "    return tweets\n",
    "\n",
    "collected_tweets = search_tweets(\"https://twitter.com/search?q=(%22Tilburg%20University%22%20OR%20%22Tilburg%20Universiteit%22)&src=typed_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://twitter.com//IamExpatJobs/status/1341409427170205697'"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "\n",
    "# add \"-trein\" to the URL (as simple as that!)\n",
    "search_tweets(\"https://twitter.com/search?q=(%22Tilburg%20University%22%20OR%20%22Tilburg%20Universiteit%22)%20-trein&src=typed_query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have undoubtedly noticed that by default Twitter search shows the so-called \"Top\" tweets. However, you can click on any of the other tabs so that it returns data from a specific type, data sorted in a given order, or from Twitter users that meet your selection. Note that you can select up to 1 tab and 1 search filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tab | Interpretation | URL suffix | \n",
    "| :----- | :------ | :------ |\n",
    "| Latest | Sort the tweets chronologically (in descending order) | `&-f=live` |\n",
    "| People | Return a list of Twitter accounts (as opposed to tweets) | `&-f=user` |\n",
    "| Photos | Filter on tweets that include an image | `&-f=image` |\n",
    "| Videos | Filter on tweets that include a video | `&-f=video` |\n",
    "\n",
    "| Search filter| Interpretation | URL suffix | \n",
    "| :----- | :------ | :------ |\n",
    "| Near you | Return tweets that are published by someone in your neighborhood | `&lf=on` |\n",
    "| From people you follow | Filter on tweets from accounts you follow | `&pf=on` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**  \n",
    "Brainstorm about search strategies you can deploy to narrow down your Twitter search and make the output of Exercise 5 more useful and actionable for the PR department. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "You may want to filter down on the latest tweets near Tilburg to separate the signal the noise since. Some public statements may require immediate action after all.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Wrap-Up\n",
    "In this section, you have learned how to use a browser emulation (Selenium) to access data that is hidden behind a login-screen or only appears once scrolling down. This opens up a whole new world of opportunities for you to explore. Some ideas to explore on your own: analyze the hashtags that get the highest engagement, plot the Twitter follower growth over time, identify upcoming influencers, and conduct market research to derive insights to grow your audience. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Instagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Click Sites Programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**  \n",
    "Now that you got a feeling for how to work with Selenium, we're going to have a brief look at how to scrape data from Instagram. In many ways, the techniques and code follow the same logic as above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we navigate towards the [Instagram account](https://www.instagram.com/tilburguniversity/) of Tilburg University with Selenium, accept the cookies, login with our own credentials (we recommend creating a separate account for scraping purposes), and close any windows that may occur (e.g. \"Save your login info\", or \"Turn on notifications\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a new driver object if you accicentally closed it (see beginning of this notebook)\n",
    "driver.get(\"https://www.instagram.com/tilburguniversity/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page structure of Instagram is slightly different from Twitter in the sense that it presents an overview of all images from which we gather the links to all separate posts (like the webshop example). Inspection of the HTML structure tells us that a link to each post can be obtained from the `<a>` tags with attribute `tabindex:\"0\"` within the `<article>` tags: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The link of the most recent post is: http://www.instagram.com//p/CJItoJODy9y/\n"
     ]
    }
   ],
   "source": [
    "# login and close pop-ups before proceeding!\n",
    "driver.get(\"https://www.instagram.com/tilburguniversity/\")\n",
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "link = soup.find(\"article\").find_all(attrs={\"tabindex\": \"0\"})[0][\"href\"]\n",
    "print(f\"The link of the most recent post is: http://www.instagram.com/{link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Get the links from all other posts! How many images/videos can you access (without scrolling down)? What does the `alt` attribute in the `img` tags tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "One of the [links](https://www.instagram.com/p/B_KalD_DHcV/) that you'll get this way, showcases a picture of the Heuvelplein in Tilburg. Although comments are listed by default, replies are only visible after clicking on `View replies`. Note that replies are not simply hidden somewhere hidden in the code, in fact, a new `<div>` block of code is added to the source code.\n",
    "\n",
    "<img src=\"images/click_instagram.gif\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, you can click on each link manually but that would take ages for posts with many replies. Hence, we seek for the class of the `View replies` element and use Selenium to click on the link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.instagram.com/p/B_KalD_DHcV/\")\n",
    "view_replies = driver.find_elements_by_class_name(\"EizgU\")[0]\n",
    "sleep(3) # wait a few seconds before the page is fully loaded\n",
    "view_replies.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Run the `.click()` command one more time, what happens then? Which replies are triggered by this click? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6**  \n",
    "Write a function `instagram_replies()` that extracts all replies for a given Instagram URL. It should return both the username and the replies. Tip: use `.next_element` to element directly after the `View replies` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reply': \"@lauke_p I'll send you a PM with more information 😊\",\n",
       "  'user': '/juliatheuws/'},\n",
       " {'reply': \"@nneratzaki sure! I work one day a week on average. So I have the rest of the week for my study and other activities. And during the exams it's almost always possible to make it work somehow. If you have more questions, don't hesitate to send me a PM! 😃\",\n",
       "  'user': '/juliatheuws/'}]"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "def instagram_replies(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    # click on links\n",
    "    buttons = driver.find_elements_by_class_name(\"EizgU\")\n",
    "    replies = []\n",
    "    \n",
    "    for button in buttons: \n",
    "        button.click()\n",
    "        sleep(1)\n",
    "    \n",
    "    # extract HTML\n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "    data = soup.find_all(class_=\"EizgU\")\n",
    "\n",
    "    # collect comments\n",
    "    for reply in data: \n",
    "        text = reply.next_element.next_element.find(class_=\"\").text\n",
    "        user = reply.next_element.next_element.find_all(\"a\")[1][\"href\"]\n",
    "\n",
    "        replies.append({\"reply\": text, \n",
    "                        \"user\": user})\n",
    "\n",
    "    return replies\n",
    "    \n",
    "instagram_replies(\"https://www.instagram.com/p/B_KalD_DHcV/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scrape Image Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**  \n",
    "In previous examples, we looked at scraping textual data from a web page. On Instagram, however, it would make sense to store the image files as well. To this end, we extract a link to the image source (`image_link`) and pass it to the `wget` library. You can name the image whatever you want (`my_image.jpg`). By default, the image is stored in your current working directory (i.e., where this notebook resides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_image.jpg'"
      ]
     },
     "execution_count": 1090,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "image_link = soup.find(attrs={\"decoding\": \"auto\"})[\"src\"]\n",
    "wget.download(image_link, \"my_image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**  \n",
    "Visit another Instagram page (with `drive.get()`), extract the HTML of that page, recreate a `soup` object, and see whether you can store the image on your local machine. Can you also scrape videos in this way? How about Instagram [carousels](https://www.instagram.com/p/CIGLXWMoPkh/) (i.e., posts that contain multiple media)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**  \n",
    "Write a function `scrape_image()` that takes an Instagram URL and returns all images associated with the post. In other words, if the post contains 3 images it should return all 3 of them. Use the post identifier (e.g., for [this](https://www.instagram.com/p/CIGLXWMoPkh/) post use `CIGLXWMoPkh`) appended by `_image[NR].jpg` (e.g., `CIGLXWMoPkh_image1.jpg`, `CIGLXWMoPkh_image2.jpg`, etc.) as the file name for the images. Skip the posts displayed under \"More posts from ...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "def scrape_image(url):\n",
    "    driver.get(url)\n",
    "   \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    \n",
    "    data = soup.find(attrs={\"role\": \"presentation\"}).find_all(attrs={\"decoding\": \"auto\"})\n",
    "    \n",
    "    for counter in range(len(data)):\n",
    "        item_link = data[counter][\"src\"]\n",
    "        file_name = url.strip('/')[-11:] + \"_image\" + str(counter + 1) + \".jpg\"\n",
    "        wget.download(item_link, file_name)\n",
    "        \n",
    "scrape_image(\"https://www.instagram.com/p/CIGLXWMoPkh/\")\n",
    "\n",
    "# note that the 3rd image is only loaded once you click on the right arrow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wrap-Up\n",
    "We have gone a long way since the Web data for dummies tutorial and learned a variety of techniques to get to the data we're after. In particular, social network services rely on scrolling and clicking interactions to achieve this. As said many times before, this is just the beginning of your scraping journey. For example, see whether you can exploit Instagram's explore function to filter down on posts with a specific [tag](https://www.instagram.com/explore/tags/tilburguniversity/)\n",
    "or [location](https://www.instagram.com/explore/locations/213125596/tilburg-netherlands/). Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
