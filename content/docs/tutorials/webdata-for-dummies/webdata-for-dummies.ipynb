{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webdata for Dummies (oDCM)\n",
    "\n",
    "*The internet offers abundant possibilities to collect data that can be used in empirical research projects and provide business value. This tutorial is a gentle introduction using web scraping and APIs for collecting data from the web. Get inspired now!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "After completion of this tutorial, students will be able to:\n",
    "\n",
    "* Explain the differences between websites and APIs\n",
    "* Retrieve web data in Python (`requests`), and save request results in HTML or JSON/TXT files for further inspection\n",
    "* Use browser control tools to develop strategies for capturing data from websites (e.g., text, numbers, pictures)\n",
    "* Select elements from websites using selectors in BeautifulSoup\n",
    "* Select elements from the JSON result set of APIs using attribute-value pairs\n",
    "\n",
    "--- \n",
    "\n",
    "## Acknowledgements\n",
    "This tutorial has been inspired by various open-access online resources, which we list for further reference at the [course website](https://odcm.hannesdatta.com/docs/about). \n",
    "\n",
    "--- \n",
    "\n",
    "## Support Needed?\n",
    "For technical issues outside of scheduled classes, please check the [support section](https://odcm.hannesdatta.com/docs/course/support) on the course website.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Web Scraping\n",
    "\n",
    "### 1.1 What is web scraping?\n",
    "\n",
    "Say that you want to capture and analyze data from a website. Of course, you could simply copy-paste the data from each page. But, of course, this manually executed job would have severe limitations. What if the data on the page gets updated (i.e., would you have time available to copy-paste the new data, too)? Or what if there are simply so many pages that you can't possibly do it all by hand (i.e., thousands of product pages)? \n",
    "\n",
    "Web scraping can help you overcome these issues __by programmatically grabbing data from the web__. Before we can extract/grab/capture elements from a website, we need a bit of background on how websites work technically, so let's focus on that first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 1.2. How websites work\n",
    "\n",
    "#### Importance\n",
    "\n",
    "It's vital to take some time to get familiar with HTML - the primary programming language used when building websites. Once we're familiar with HTML (and the structure of websites), we can rapidly navigate complex websites to extract the elements we're interested in (e.g., price, product categories, ...). In other words: to reach our end goal, we do have to give you some technical details first.\n",
    "\n",
    "So, here we go: A web page consists of various text files, each one with its style, formatting, and syntax. These files each serve a specific purpose:\n",
    "\n",
    "- `.html` (HyperText Markup Language) files give structure to a page (e.g., where's the menu?, which content to show (e.g., text, tables)?)\n",
    "- `.css` (Cascading Style Sheet) files determine how the page looks (e.g., which color do headers have? what's the font used for text in a paragraph?)\n",
    "- `.js` (JavaScript) files add interactivity (e.g., button animations)\n",
    "\n",
    "#### Let's try it out\n",
    "Check out this simple [example](https://codepen.io/rcyou/pen/QEObEk/). The site shows the source code of a site (`.html`, `.css`, and `.js`) in an online editor, along with a rendered (\"viewable\") version of the site. Once you make changes to the code, the site gets automatically updated.\n",
    "\n",
    "<img src=\"images/codepen.png\" align=\"left\" width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 \n",
    "Just to get a feeling for how things work, let's make the following changes in the [CodePen snippet](https://codepen.io/rcyou/pen/QEObEk/): \n",
    "1. Change the text between the `<h1>` tags to `I am a purple of size 3em`. \n",
    "2. Change the `h1` font-size to `3em` and the color to purple (add `color: purple;` below `margin-bottom`).  \n",
    "3. Remove the JavaScript code. What happens now when you click the blue button?\n",
    "\n",
    "---\n",
    "\n",
    "#### Solutions\n",
    "Clicking the button should no longer trigger the script to hide the paragraph text.\n",
    "\n",
    "<img src=\"images/purple_headline.png\" align=\"left\" width=60%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### 1.3 Advancing HTML skills\n",
    "\n",
    "#### Importance\n",
    "\n",
    "Most HTML elements are represented by a pair of tags - an opening tag and a closing tag. \n",
    "\n",
    "For example, a table starts with `<table>` and ends with `</table>`. \n",
    "\n",
    "The first tag tells the browser: \"Hey! I got a table here! Render it as a table, so it displays nicely on the site.\" \n",
    "\n",
    "The closing tag (note the forward-slash!) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested more HTML tags representing rows (`<tr>`) and cells (`<td>`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This what the rendered HTML table looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "HTML elements can have any number of\n",
    "\n",
    "- __attributes__, such as IDs, which *uniquely* identify elements\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\">\n",
    "```\n",
    "\n",
    "- __classes__, which identify a *type* of an element (contrary to ids, a class can be used more than once) --\n",
    "\n",
    "```html\n",
    "<table class=\"striped-table\">\n",
    "```\n",
    "\n",
    "- and __styles__, which define how specific elements *appear* (e.g. the width of the table) --\n",
    "\n",
    "```html\n",
    "<table style=\"width:95%;\">\n",
    "```\n",
    "\n",
    "As you may already have noticed, we use spaces (or tabs) to separate the elements from one another (the geeks among us will call this \"indentation\") to provide structure and improve readability.\n",
    "\n",
    "Yes, that's right. *Improve readability*.\n",
    "\n",
    "Code may look complex to read at first, but when you take a closer look at it, it boils down to simple English, following a particular structure (also known as syntax).\n",
    "\n",
    "For example, the `<table>` tag is placed farther to the right than the `<html>` tag indicates that the table is nested within the HTML block.\n",
    "\n",
    "This may be a lot to take in if you're entirely new to HTML, but don't worry, as the goal of this section is not to teach you how to code from scratch but rather to teach you what HTML is and why it is relevant for web scraping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Double-click on the rendered table below to edit its HTML structure. Try to change some simple things, e.g., the text. Re-run the cell (Shift + Enter, or click the Run button in Juypyter Notebook). Watch your changes come alive!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Please finish the exercises below. After each change, rerun the cell.\n",
    "\n",
    "Important: don't make too many changes at once. Always proceed in small steps to see whether your code (still) works!\n",
    "\n",
    "1. Add another row in the table above to become a 2 (columns) x 4 (rows) table. That is 3 regular rows and 1 table header row.\n",
    "2. Fill the cells with the corresponding text labels (e.g., Row 3, Column A). \n",
    "3. Change the table width to `50%` so that the table becomes narrower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Make your changes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions**\n",
    "<html>\n",
    "    <table id=\"example-table\" class=\"striped-table\" style=\"width: 50%\">\n",
    "        <tr> <!-- Header -->\n",
    "            <td>Column A</td>\n",
    "            <td>Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 1 --->\n",
    "            <td>Row 1, Column A</td>\n",
    "            <td>Row 1, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 2 --->\n",
    "            <td>Row 2, Column A</td>\n",
    "            <td>Row 2, Column B</td>\n",
    "        </tr>\n",
    "        <tr> <!-- Row 3 --->\n",
    "            <td>Row 3, Column A</td>\n",
    "            <td>Row 3, Column B</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### 1.4 Finding content in a website's source code\n",
    "\n",
    "#### Importance\n",
    "\n",
    "Alright, we've now covered tables (`<table>`). However, there are hundreds of different tag words in HTML, and it's impossible to memorize all of them. That's why developers use a pretty handy tool to *inspect the source* of a website directly in the browser. From now onwards, we recommend you to use Chrome (in Safari and Mozilla, things look slightly different, and we can't cover those, unfortunately.)\n",
    "\n",
    "Suppose you have identified an element you want to capture (e.g., a price or the name of a product). You can \"ask\" your browser for the specific HTML tag of that object (so it becomes easier to capture that element later). \n",
    "\n",
    "#### Let's try it out \n",
    "\n",
    "\n",
    "How does it work? Start by inspecting specific elements on the page by *right-clicking on the page* and selecting *\"Inspect\"* from the context menu that pops up. Then, hover over elements in the \"Elements\" tab to highlight them on the page. This can be super helpful when you're trying to figure out how to (uniquely) identify the element you want to scrape.\n",
    "\n",
    "Check out the HTML structure of this fictitious [online bookstore](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html). Each of the 1000 books has its page, which shows the title, stock level, star rating, product description, and a table with other product information. Note that the prices and ratings are randomly generated, and therefore the figures on your screen may deviate from the ones below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/inspect.gif\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the screenshot above, we've selected the book title (\"A Light in the Attic\"), right-clicked on it, and chose \"Inspect.\" The same text is highlighted in blue in the HTML code below. \n",
    "\n",
    "Try it out yourself!\n",
    "\n",
    "The `<h1>` and `</h1>` tags surrounding the title indicate that this text is a header on the web page. Move your pointer down to the line below (`<p class=\"price_color\">£51.77</p>`), and you'll see that on the top screen, it now highlights the price (rather than the title) of the book. This way, you can quickly investigate any webpage. \n",
    "\n",
    "Also, try this out yourself!\n",
    "\n",
    "As we discussed earlier, tags can be nested within other tags. This also becomes clear from the screenshot below, in which the small gray triangles (▶) indicate that there is code hidden within these blocks. Click on them to expand the code, see what's inside, and click again to collapse them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/html_structure.png\" align=\"left\" width=70%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "1. Use the inspect tool to find the HTML element that constitutes the table header \"**Number of reviews**\" at the bottom of this [page](https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html).\n",
    "2. Look up how many elements on the page are associated with the class `sub-header` (within the Inspector screen, use `Ctrl+F` on a PC or `⌘+F` on Mac to search)\n",
    "3. You can make local (only on your computer) changes to the web page by double-clicking in the inspector and swapping the code for something else (yes, you can overwrite what's already written there!). Change the price of the book to £39.95 and assign it a five star-rating. \n",
    "4. After making the changes in 3.), refresh the page (reload it). What happens (and why)?\n",
    "\n",
    "\n",
    "*A \"faked\" price and star-rating*\n",
    "\n",
    "<img src=\"images/exercise_inspector.png\" width=40% align=\"left\"  style=\"border: 1px solid black\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. The `<th>` (table header) tags enclose the text \"Number of reviews.\" \n",
    "2. 3 elements are associated with the class `sub-header` (product description, product information, reviews)\n",
    "3. The star rating can be changed from the class attribute to `star-rating Five`. Once you refresh the page, the original (unedited price and star rating) appears again.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Loading a website's source code into Python\n",
    "\n",
    "#### Importance\n",
    "\n",
    "Alright. Up to this moment, we've learned about HTML and fiddled around with a website's source code. But we finally want to understand how we can load a website's source code into Python.\n",
    "\n",
    "Rather than (manually) using the Inspector, we now automate these tasks using Python's `requests` library. Libraries are \"extensions\" to Python, but most of them are not loaded by default. So let's import the library using `import requests`.\n",
    "\n",
    "The total source code of the website, by the way, contains over 9000 characters. Therefore, we only print out the product description here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please (re)run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <div id=\"product_description\" class=\"sub-header\">\n",
      "            <h2>Product Description</h2>\n",
      "        </div>\n",
      "        <p>It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more</p>\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# make a get request to the \"A Light in the Attic\" webpage\n",
    "url = 'https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html'\n",
    "book_request = requests.get(url)\n",
    "\n",
    "# return the source code from the request object\n",
    "book_source_code = book_request.text\n",
    "\n",
    "# print out part of the source code\n",
    "print(book_source_code[5710:6860])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Now it's your turn! Use slicers in the code below so that it prints out the __title of the book__: *A Light in the Attic* (without all other stuff)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_source_code[...:...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "A Light in the Attic\n",
      "A Light in the Attic\n"
     ]
    }
   ],
   "source": [
    "# one of these solutions (multiple solutions possible)\n",
    "print(book_source_code[3319:3339]) \n",
    "print(book_source_code[4479:4499])\n",
    "print(book_source_code[5872:5892])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Finding content in a website's source code using `BeautifulSoup` \n",
    "\n",
    "#### Importance\n",
    "\n",
    "As you've probably noticed, it was quite a hassle to extract the right elements from the page in the previous exercise. Fortunately, there is a much better way, namely by using __CSS locators__ (these are beyond the scope of this tutorial, though). For now, we'll show you how you simplify your search for text in a website's source code.\n",
    "\n",
    "*BeautifulSoup* is a fantastic, open-source Python library that allows you to navigate and extract data from HTML files. BeautifulSoup does NOT gather information from the web itself (for this, we use `requests`, as above). \n",
    "\n",
    "#### Let's try it out\n",
    "First, we'll send a request to a webpage to load the website's source code into memory, and then we send that code over to Beautifulsoup to extract the desired information. \n",
    "\n",
    "In the code snippet below, we import the package and turn the `book_source_code` (the HTML code from the \"A Light in the Attic\" webpage we used earlier) BeautifulSoup object. Once converted (\"parsed\"), we can easily navigate the code by *tag names* (remember tags from above?!). \n",
    "\n",
    "Since we know that the title is surrounded by `<h1>` tags (see Google Inspector screenshot above), we use `soup.find('h1')` to print out the title of the book. Do you see how much easier that is, compared to specifying the *exact position* using slicers?\n",
    "\n",
    "Please run the following cells to see things in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>A Light in the Attic</h1>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(book_source_code)\n",
    "print(soup.find('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find()` method will always print out the first matching element that it finds. For example, the web page has two `<h2>` elements which contain the \"Product Description\" and \"Product Information\" subheaders.  Only the first one will be returned by `.find()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Product Description</h2>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('h2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture all matching `<h2>` elements you use the `find_all()` method like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h2>Product Description</h2>, <h2>Product Information</h2>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('h2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it now returns a list of elements (`[element1, element2]`), so to access individual elements you need to apply indexing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>Product Description</h2>\n",
      "<h2>Product Information</h2>\n"
     ]
    }
   ],
   "source": [
    "# obtain first h2 element \n",
    "print(soup.find_all('h2')[0])\n",
    "\n",
    "# obtain second h2 element\n",
    "print(soup.find_all('h2')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both subheaders are still surrounded by `<h2>` and `</h2>` tags. To get rid of them, append `.get_text()` to your code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Description\n"
     ]
    }
   ],
   "source": [
    "# sub header without h2 tags\n",
    "print(soup.find_all('h2')[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "1. Retrieve the website's source code, and capture the content of the HTML table listing the product information (UPC, type, price, tax, etc.). Use the \"A Light in the Attic\" book site, and `BeautifulSoup`. \n",
    "\n",
    "The output should look like this (\"Â\" has to do with the GBP-pound symbol): \n",
    "\n",
    "```html\n",
    "<table class=\"table table-striped\">\n",
    "    <tr>\n",
    "        <th>UPC</th><td>a897fe39b1053632</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Product Type</th><td>Books</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Price (excl. tax)</th><td>Â£51.77</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Price (incl. tax)</th><td>Â£51.77</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Tax</th><td>Â£0.00</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Availability</th>\n",
    "        <td>In stock (22 available)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Number of reviews</th>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "\n",
    "2. Extract the tax amount from the table. Tip: you can chain `.find()` and `find_all()` statements, for example: `.find('body').find_all('h1')` returns all `<h1>`s in the `body`. Your code should return the following output: \n",
    "\n",
    "```html\n",
    "Â£0.00 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"table table-striped\">\n",
      "<tr>\n",
      "<th>UPC</th><td>a897fe39b1053632</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Product Type</th><td>Books</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Price (excl. tax)</th><td>Â£51.77</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Price (incl. tax)</th><td>Â£51.77</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Tax</th><td>Â£0.00</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Availability</th>\n",
      "<td>In stock (22 available)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th>Number of reviews</th>\n",
      "<td>0</td>\n",
      "</tr>\n",
      "</table>\n",
      "Â£0.00\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "print(soup.find('table')) # or print(soup.find_all('table')[0])\n",
    "\n",
    "# Question 2\n",
    "print(soup.find('table').find_all('tr')[4].find('td').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've just learned the first steps in collecting online data from websites! Along with boosting your \"geek\"-factor (wait till you show this to your friends!), you've gained an intuition on how websites are built up (HTML, CSS, JS), how source code translates into a rendered (visual) website (or, in other words, you know how to spoof websites - now, take screenshots and show *that* to your friends...), how websites can be loaded into Python, and how you can use `BeautifulSoup` to extract HTML elements by using their tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 2. Application Programming Interface (API)\n",
    "\n",
    "\n",
    "### 2.1 What is an API?\n",
    "\n",
    "An equally important data collection method is called Application Programming Interface (API). That's a mouthful, but in essence, it is nothing more than a version of a _website intended for computers, rather than humans, to talk to one another_. \n",
    "\n",
    "APIs are everywhere, and most are used to provide...\n",
    "- data (e.g., retrieve a user name and demographics), \n",
    "- functions (e.g., start playing music from Spotify, turn on your lamps in your \"smart home\"), or \n",
    "- algorithms (e.g., submit an image, retrieve a written text for what's *on* the image).\n",
    "\n",
    "In what follows, we'll introduce you to the API of [Reddit](https://www.reddit.com), a popular American social news aggregation and discussion site that is sometimes described as the *front page of the internet*. Reddit gives you an up to date view on what's happening around the world and is based on the principle that the community of around 1 billion users decides what is newsworthy and what's not through a voting system. \n",
    "\n",
    "You can think of Reddit upvotes as Facebook likes. Posts are arranged based on the number of votes, and those with many upvotes are featured on the homepage. The grey number next to each post represents the sum of votes (= upvotes - downvotes).\n",
    "\n",
    "\n",
    "<img src=\"images/reddit_homepage.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How APIs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "APIs work very similar to websites. At the core, instead of obtaining the source code of a (rendered) web site, you obtain code that computers can easily understand to process the content of a website. APIs provide you with simpler and more scalable ways to obtain data, so you really have to understand how they work.\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Consider the screen shot above (a view of the Reddit website). Here's an example of how the output of the Reddit API (click on it to view it in your browser):\n",
    "\n",
    "https://www.reddit.com/r/science/comments/k0bjqt/study_finds_users_not_notifications_initiate_89.json\n",
    "\n",
    "<img src=\"images/api_example.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A few things stand out right away:*\n",
    "\n",
    "- the output only contains text, which is structured according to a data structure (e.g., list (`[]`) and dictionary (`{}`)), \n",
    "- there's no human interface with buttons, menus, and links, yet...\n",
    "- you can access it like any other website by filling out the URL in your browser (`reddit.com/r/science/...` in this example).\n",
    "\n",
    "In fact, the API output above corresponds to the (visual) Reddit thread, which you can open here:\n",
    "\n",
    "https://www.reddit.com/r/science/comments/k0bjqt/study_finds_users_not_notifications_initiate_89\n",
    "\n",
    "For example, look at the third and fourth line from above, which states the `title` of the post you can also see below on the rendered website.  \n",
    "\n",
    "<img src=\"images/reddit.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tip:*\n",
    "If you have taken a look at the API output, you may conclude that making sense of raw JavaScript Object Notation (JSON) is easier said than done. Fortunately, this [plugin](https://chrome.google.com/webstore/detail/json-viewer/gbmdgpbipfallnflgajpaliibnhdgobh) automatically formats and highlights the output such that it's easier to digest. For the following exercise, we therefore highly recommend installing the Chrome plugin. \n",
    "\n",
    "Install it, and view the output again. That's much better, right? (Alternatively, you can copy-paste the JSON output into this [online viewer](http://jsonviewer.stack.hu) and inspect the \"Viewer\" tab).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Navigate through the JSON tree structure of the post above and anwer the following questions:\n",
    "\n",
    "1. At the parent level you find two dictionaries at line 5 and 197 (i.e., the blue arrows). Collapse the content and describe in your own words what each dictionary represents. How does it relate to the Reddit HTML page? \n",
    "\n",
    "<img src=\"images/reddit_api.gif\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The first comment is from the post author (fotogneric) and has gathered the most points. How many downvotes did this comment get (you find the answer in the JSON output)? \n",
    "\n",
    "3. Suppose that you want to extract the date and time each comment was created. What path do you need to navigate? \n",
    "\n",
    "(Note that times are often registered in UTC format, a globally interchangeable time representation (also known as Epoch time). More specifically, it is the number of seconds elapsed since January 1, 1970. It can be used as a universal time scale around the world. Copy-paste the UTC time to an online [epoch converter](https://www.epochconverter.com), and check whether it corresponds with the date and time on the webpage).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. The dictionary that starts at line 5 contains data on the post (title, subreddit, upvote ratio, thumbnail/image, link to article). The other dictionary stores the comments of the post (author, body text, timestamp). \n",
    "2. At the moment of writing this solution (December 2020) the post has 0 downvotes (`'downs': 0`).\n",
    "3. The `created` key stores a large number (e.g., 1606274053) that can be translated into a date and time (for this example: 25 November 2020 03:14 GMT). The corresponding path for the timestamp of the first comment is: `request[1]['data']['children'][0]['data']['created']` (a written description that follows these directions also suffices: first, you take the 2nd element `[1]` in the list, then you choose the `data` key, etc.).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Retrieve data from an API endpoint\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We now proceed to *downloading* data from an API. But since APIs are mostly provided via paid subscriptions, all we can offer here is a nerdy example *of a free API*, which - unfortunately - isn't even remotely related to the field of marketing. \n",
    "\n",
    "We proudly present to you... __[icanhazdadjoke.com](https://icanhazdadjoke.com)!__ ...which is the largest selection of *dad jokes on the internet* (yes, we also didn't know that existed!).\n",
    "\n",
    "---\n",
    "\n",
    "*Background: Free versus paid APIs*\n",
    "\n",
    "Paid APIs require their users to authenticate themselves. Think of an authentication key as a \"key to unlock the service.\" Web services use such authentication keys to track whether you're allowed to use the API and how much you use it. This offers numerous opportunities for API business models, in which, for example, the service employs a pay-by-request (or by 1,000 requests) model.\n",
    "\n",
    "---\n",
    "\n",
    "*Back to Icanhazdadjoke.com...*\n",
    "\n",
    "We picked this web site because we don't have to use any authentication token, and there's no limit to retrieving data. But, how does the website work, and why is there an API?\n",
    "\n",
    "Every time you visit the site, the site shows a *random joke*. From a technical perspective, each time a user opens the site, a little software program on the server makes an API call to the daddy joke API to draw a new joke to be displayed. The designers have split the showing of information (website) from the actual content (the jokes, available through the API). This offers the opportunity to provide the data in two ways: an excellent visual representation of dad jokes (the website) and a service for drawing jokes programmatically to embed in other software products.\n",
    "\n",
    "Sounds familiar? Yep! Facebook and Instagram do precisely the same. Instead of tying in their technology with the website, they have split the visual representation from the actual content. This allows social media networks to monetize their data in other ways (e.g., by having advertisers programmatically access the Facebook API to learn about potential targets for their ad campaigns).\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Try out to generate a [random joke](http://icanhazdadjoke.com) on the website... (click the link)\n",
    "\n",
    "\n",
    "<img src=\"images/icanhazdadjoke.gif\" width=70% align=\"left\"  style=\"border: 1px solid black\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we retrieve that data via the service's API?\n",
    "\n",
    "Like for webscraping (where we stored the HTML source code of the website in the `book_request` variable), we proceed with the `requests` library. The only difference is we need to add a so-called header (so that the API knows it's talking to a computer! In Python: `headers={\"Accept\": application/json}\"`).\n",
    "\n",
    "Each `joke_request` response from the API contains three attributes: \n",
    "* `id` = a unique identifier for each joke\n",
    "* `joke` = the text of the joke\n",
    "* `status` = the HTML status code (200 indicates a successful request)\n",
    "\n",
    "Try to spot those attributes in the printed JSON response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'AQn3wPKeqrc', 'joke': 'It was raining cats and dogs the other day. I almost stepped in a poodle.', 'status': 200}\n"
     ]
    }
   ],
   "source": [
    "# request JSON output from icanhazdadjoke API\n",
    "url = \"https://icanhazdadjoke.com\"\n",
    "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "joke_request = response.json() \n",
    "print(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "1. What happens if you run the cell above again? Why is that? \n",
    "2. Turn off your WiFi and try running the cell again. What happens this time? \n",
    "3. Turn on your WiFi again and revise the code snippet above, so that it stores the text of 10 jokes in a list. You can extract the text of the joke as follows: `joke_request['joke']` (tip: use a for-loop). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. Another random joke is generated, so the `id` and `joke` change every time. \n",
    "2. A connection error occurs because the `requests` package could not establish a connection with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "jokes = [] \n",
    "\n",
    "for counter in range(10):\n",
    "    url = \"https://icanhazdadjoke.com\"\n",
    "    response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "    joke_request = response.json() \n",
    "    jokes.append(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 APIs versus web scrapers\n",
    "\n",
    "Now that you understand what APIs are, you may rightfully wonder: why should I learn APIs when I could scrape the elements from the website instead (like the book webshop)?\n",
    "\n",
    "One of the major advantages of APIs is that you can directly access the data you need *without all the hassle of selecting the right HTML tags*. Another advantage is that you can often customize your API request (e.g., the first 100 comments or only posts about science), which may not always be possible in the web interface. \n",
    "\n",
    "Last, using APIs is legitimized by a web site (mostly, you will have to pay a license fee to use APIs!). So it's a more stable and legit way to retrieve web data compared to web scraping. That's also why we recommend using an API whenever possible. \n",
    "\n",
    "In practice, though, APIs really can't give you all the data you possibly want, and web scraping allows you to access complementary data (e.g., viewable on a website or somewhere hidden in the source code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing this section, you should not only have learned a couple of new dad jokes, but you should also be able to explain to your parents what an API is and give practical examples of how consumers - as well as companies - may benefit from using them. Furthermore, you should be able to request data from various APIs  (even if it's one you have never seen before!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
